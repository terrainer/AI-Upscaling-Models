# GENERATE TIME: Tue Jan 30 02:27:59 2024
# CMD:
# train.py --auto_resume -opt options/4x_SPANkendata_FineTune3.yml


name: 4x_SPANkendata_FineTune3
model_type: default
scale: 4
num_gpu: 1
use_amp: true
bfloat16: true
compile: false
#manual_seed: 1024

datasets:
  train:
    type: paired
    dataroot_gt: 'D:\Frankendata\HR'
    dataroot_lq: 'D:\Frankendata\LR_Mix'
    #meta_info: 'datasets/meta_info.txt'
    io_backend:
      type: disk

    gt_size: 192
    batch_size: 10
    use_hflip: true
    use_rot: true
    num_worker_per_gpu: 4
    dataset_enlarge_ratio: 1

  val:
    name: val_1
    type: paired
    dataroot_gt: 'D:\validation\4xLSDIR_Trimmed_val\HR'
    dataroot_lq: 'D:\validation\4xLSDIR_Trimmed_val\LR\'
    io_backend:
      type: disk
val:
  val_freq: 5000
  save_img: true
  metrics:
    psnr:
      type: calculate_psnr
    ssim:
      type: calculate_ssim

path:
  pretrain_network_g: 'E:/neosr/experiments/4x_SPANkendata_FineTune2/models/net_g_110000.pth'
  resume_state: ~
  #strict_load_g: false # do not uncomment, read docs

network_g:
  type: span
  rgb_mean: [0.4685, 0.45367, 0.4090]

network_d:
  type: unet

train:
  optim_g:
    type: prodigy
    lr: 1.0
    d_coef: 0.25
  optim_d:
    type: prodigy
    lr: 1.0
    d_coef: 0.25

  scheduler:
    type: none

  total_iter: 200000
  warmup_iter: -1 # no warm up

  # losses
  pixel_opt:
    type: HuberLoss
    loss_weight: 0.1
  perceptual_opt:
    type: PerceptualLoss
    perceptual_type: dual
    layer_weights:
      'conv1_2': 0.1
      'conv2_1': 0.1
      'conv2_2': 0.1
      'conv3_3': 1
      'conv3_4': 1
      'conv4_4': 1
      'conv5_4': 1
    perceptual_weight: 1.2
    style_weight: 0.0
    criterion: patch
    use_std_to_force: true
    perceptual_patch_weight: 1.0
    perceptual_kernels: [4, 8]
  gan_opt:
    type: GANLoss
    gan_type: vanilla
    loss_weight: 0.3
  color_opt:
    type: colorloss
    loss_weight: 0.5
    criterion: huber
  gradvar_opt:
    type: gradientvarianceloss
    patch_size: 8
    loss_weight: 1.0
  patch_opt:
    type: patchloss3dxd
    use_std_to_force: false
    kernel_sizes: [ 7, 11, 15 ]
    loss_weight: 1.0
  ldl_opt:
    type: HuberLoss
    loss_weight: 1.0
  ff_opt:
    type: focalfrequencyloss
    loss_weight: 2.0
    ave_spectrum: true
    log_matrix: false
    batch_matrix: false

logger:
  print_freq: 100
  save_checkpoint_freq: 1000
  use_tb_logger: true
  #wandb:
  #  project: ~
  #  resume_id: ~

# dist training settings
#dist_params:
#  backend: nccl
#  port: 29500
